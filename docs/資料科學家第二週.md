# 資料科學家第二週

## 分類問題與決策樹

* 當目標變數為**離散型**時則使用**回歸分析**
* 當目標變數為**連續型**時則使用**分類決策**

| 編號 | 貸款 | 婚姻狀況 | 應稅收入 | 違約 |
|---|---|---|---|---|
||*屬性集(x1)*|*屬性集(x2)*|*屬性集(x3)*|*類別(y)*|
|1|是|單身|125K|否|
|2|否|已婚|100K|否|
|3|否|單身|70K|否|
|4|是|已婚|120K|否|
|5|否|離異|95K|是|
|6|否|已婚|60K|否|
|7|是|離異|220K|否|
|8|否|單身|85K|是|
|9|否|已婚|75K|否|
|10|否|單身|90K|是|

### 分類問題的input output
> 分類問題最主要目標為，利用這些輸入的屬性集x，透過分類的模型
> ( 決策樹 or K最近鄰法 or SVM or 類神經網路 .etc)
> 來幫助顧客或分析師預測出這筆資料最後的類別y，即 F(X) → Y

## 分類三步驟
1. 取得**資料完整**的訓練資料集
2. 建立分類模型 ( 分類技術 )
  - 分類樹/決策樹 ( Classification Tree / Decision Tree )
  - K最近鄰近點分類法 ( K-Nearest Neighbor )
  - 類神經網路
  - 原始貝氏模型與貝氏網路
  - 支援向量機
3. 使用測試資料集去驗證分類模型的正確率

## 決策樹的建構
1. 使用 *Hunt's 演算法* 來建構二元樹or多元樹
  - 如果D{t}為**空集合**，則t為標記預設Y{d}的葉節點
  - 如果D{t}包含了**相同的類別**，則t為標記Y{t}的葉節點
  - 如果D{t}包含超過一種類別，將會再使用其他變數切分為更小的子集合，重複此動作，直到D{t}裡只有一種類別
2. 決策樹的特性
  - 使用不同的變數順序，會導致不同結構的樹
  - 不同結構的樹，會有不同的效益
    + 正確率不同
    + 速度不同
    + 決策樹的分析規則、解釋不同
  - 所以要在建構樹之前找到好的變數順序
  - 變數順序必須完全排列組合完畢才能找到最佳解 ( 三種變數，則有 *3!* 種排列組合 )
3. 使用指標 ( 亂度 )
  - 吉尼係數 ( Gini Index )
  > 此項係數愈大，表示所得分配不均等的程度愈高，反之，係數愈小，表示不均等的程度愈低
  - 吉尼係數的分割 ( Split Gini index )
  > 追求同類別資料個數較大較純的分割，各自算完各自的節點亂度，然後乘上數量權重，就是最後的亂度

> 依照屬性集內之變數以特定的順序，一個一個套入Hunt's演算法，並建構出決策樹，變數的先後排序將影響到整體決策樹之效益，故使用指標來判定變數之順序
> 1. 選擇變數
> 2. 計算指標
> 3. 建構決策樹

## 補充
### 順序尺度
資料本身為有序性資料
* 順序尺度內的離散型變數：
  - 我醜醜 < 我普普 < 我好帥
  - 好酸 < 略酸 < 普通 < 微甜 < 好甜
* 非順序尺度內的離散型變數：
  - 豪華型車、家庭型車、運動型車
  - 圓形、方形、三角形、橢圓形、多邊形
* 順序尺度內的連續型變數：
  - 1 < 2 < 3 < 4
  - 90K < 100K < 120K < 125K < 220K

![計算吉尼係數(名目變數)](img\資料科學家\計算吉尼係數_名目變數.jpg)
![計算吉尼係數2(名目變數)](img\資料科學家\計算吉尼係數2_名目變數.jpg)
![介武的學習筆記簿](img\資料科學家\介武的學習筆記簿.jpg)
