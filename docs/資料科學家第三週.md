# 資料科學家第三週

## 使用Excel計算連續型資料的切割點之Gini Index
切割值97的Gini Index是0.3，是全部裡面最小的，也就是亂度最低，因此選擇97為切割點為最佳

![使用Excel計算連續型資料的切割點](C:\xampp\htdocs\workplace\Blog\docs\img\資料科學家\使用Excel計算連續型資料的切割點.png)

## [指標] 熵(ㄓㄜˊ ㄉ一 Entropy)
* 目的是 **測量一個節點的同質性**

![Entropy](C:\xampp\htdocs\workplace\Blog\docs\img\資料科學家\Entropy.png)

## 資訊獲得量 (Information Gain)
將父節點的Entropy減去各個子節點的**權重**Entropy
* 選擇可以減少最多亂度的分割
  - Gain極大化
  - 使用ID3、C4.5
* 缺點
  - 會因為要一直分割，造成很多節點，而每個節點都小而純，如：學號

![如何計算資訊獲得量](C:\xampp\htdocs\workplace\Blog\docs\img\資料科學家\如何計算資訊獲得量.jpg)

範例資料集中，各個變數所計算出來的資訊獲得量
* 天氣 0.246
* 溫度 0.029
* 濕度 0.151
* 風   0.048

> 因為天氣的資訊獲得量最高，代表這個變數越好

## 分類錯誤率
*最簡單、最原始、效果最差*
![如何計算分類錯誤率](C:\xampp\htdocs\workplace\Blog\docs\img\資料科學家\如何計算分類錯誤率.jpg)


## 指標優劣
** Entropy > Gini Index > 分類錯誤率 **

![三種指標比一比](C:\xampp\htdocs\workplace\Blog\docs\img\資料科學家\三種指標比一比.jpg)

## 貪婪法則
一旦變數很多，則排列組合會很龐大，不可能把所有組合的樹都建造出來

## 何時要 break out
* 所有資料都屬於相同類別
* 所有資料都有相似的屬性值
* 使用參數指標提前終止

## 分類樹的優勢
* 建樹成本低
* 可以快速地將為之資料分類
* 人腦容易解釋小型樹
* 對於簡單的資料集，精確度可媲美其他分類技術
